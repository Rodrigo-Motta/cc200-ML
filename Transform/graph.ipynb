{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "ec5c0a04-5483-4caa-a068-9e06fc2c63c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(rc={'image.cmap': 'coolwarm'})\n",
    "\n",
    "#from numba import jit,prange\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "SMALL_SIZE = 8\n",
    "MEDIUM_SIZE = 10\n",
    "BIGGER_SIZE = 20\n",
    "\n",
    "plt.rc('font', size=MEDIUM_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=BIGGER_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "cca6cf18-d292-4c44-a147-65c77b9aba54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_matrix(size, corr):\n",
    "    corr_matrix = np.zeros((size,size))\n",
    "    cont = 0\n",
    "\n",
    "    for i in range(size):\n",
    "        for j in range(i, size):\n",
    "            if i == j:\n",
    "                corr_matrix[i,j] = 0#float('nan')\n",
    "\n",
    "\n",
    "            else:\n",
    "                corr_matrix[i,j] = corr[cont]\n",
    "                corr_matrix[j,i] = corr[cont]\n",
    "                cont += 1 \n",
    "    return corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "43d0bc9e-d442-479e-a12c-e6e068f5b92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(fisher):\n",
    "    if fisher == True:\n",
    "        df = pd.read_csv(r'/Users/rodrigo/Post-Grad/CC400/corr_matrices_fisher200.csv',index_col=['Institution','Subject'])\n",
    "        phenotypic = pd.read_csv(r'/Users/rodrigo/Post-Grad/CC400/phenotypic200.csv',index_col=['Institution','Subject'])\n",
    "    else:\n",
    "        df = pd.read_csv(r'/Users/rodrigo/Post-Grad/CC400/corr_matrices200.csv',index_col=['Institution','Subject','Run'])\n",
    "        phenotypic = pd.read_csv(r'/Users/rodrigo/Post-Grad/CC400/phenotypic200.csv',index_col=['Institution','Subject'])\n",
    "    return df,phenotypic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "12920c28-3874-464b-acd6-2da7c2303fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, phenotypic = import_data(fisher=True)\n",
    "df = df.join(pd.DataFrame(phenotypic.Age), how='left')\n",
    "df = df.join(pd.DataFrame(phenotypic['ADHD Measure']), how='left')\n",
    "df = df.join(pd.DataFrame(phenotypic['Gender']), how='left')\n",
    "\n",
    "#df = ((df.reset_index()).drop(columns=['Institution', 'Subject','Run'])).dropna(subset=[str(x) for x in range(0,61425)])\n",
    "df = ((df.reset_index()).drop(columns=['Institution', 'Subject'])).dropna(subset=[str(x) for x in range(0,17954)])\n",
    "df = df.dropna(axis=0, subset=['Gender'])\n",
    "y = df.Gender\n",
    "X = df.iloc[:,:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "4aa9f44a-0853-4384-b984-6b46a357cdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.15, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "4ae6545f-ae73-4e9a-9521-c84e193aad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "\n",
    "for i in range((X_train.shape[0])):\n",
    "    adj = correlation_matrix(190,X_train.iloc[i,:].values)\n",
    "    np.fill_diagonal(adj,0)\n",
    "    adj = torch.from_numpy(adj).float()\n",
    "    edge_index, edge_attr = dense_to_sparse(adj)\n",
    "    train_data.append(Data(x=adj, edge_index=edge_index, edge_attr=edge_attr, y=torch.tensor(int(y_train.iloc[i]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "15423ce4-45b9-42d4-8a3c-94e39c72ad61",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "\n",
    "for i in range((X_test.shape[0])):\n",
    "    adj = correlation_matrix(190,X_test.iloc[i,:].values)\n",
    "    np.fill_diagonal(adj,0)\n",
    "    adj = torch.from_numpy(adj).float()\n",
    "    edge_index, edge_attr = dense_to_sparse(adj)\n",
    "    test_data.append(Data(x=adj, edge_index=edge_index, edge_attr=edge_attr, y=torch.tensor(int(y_test.iloc[i]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "e0c59bc2-b6f7-45a1-96b9-c9c891e6f301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[190, 190], edge_index=[2, 35910], edge_attr=[35910], y=0)\n"
     ]
    }
   ],
   "source": [
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "49767bb6-2c5f-4180-9165-f9d046ad7082",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(test_data, batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "07d45df4-28f8-4a4f-9b24-0449e9c2b0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for batch in train_loader:\n",
    "    #print('num_nodes',batch.num_nodes)\n",
    "    #print(batch.x.shape)\n",
    "    #print(data.edge_index.shape)\n",
    "    #print(data.batch)\n",
    "    #print('num_graphs', batch.num_graphs)\n",
    "    # data = data.to(device)\n",
    "    # optimizer.zero_grad()\n",
    "    # out = model(data.x, data.edge_index, data.batch)\n",
    "    # loss = F.cross_entropy(out, data.y)\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "    # total_loss += float(loss) * data.num_graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "c89df864-e874-4177-9a0c-094ddd99da03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as func\n",
    "from torch_geometric.nn import ChebConv, global_mean_pool\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    \"\"\"GCN model(network architecture can be modified)\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_features,\n",
    "                 num_classes,\n",
    "                 k_order,\n",
    "                 dropout=.3):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.p = dropout\n",
    "\n",
    "        self.conv1 = ChebConv(int(num_features), 128, K=k_order)\n",
    "        self.conv2 = ChebConv(128, 64, K=k_order)\n",
    "        self.conv3 = ChebConv(64, 32, K=k_order)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(32, int(num_classes))\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        batch = data.batch\n",
    "        \n",
    "        x = func.relu(self.conv1(x, edge_index, edge_attr))\n",
    "        x = func.dropout(x, p=self.p, training=self.training)\n",
    "        x = func.relu(self.conv2(x, edge_index, edge_attr))\n",
    "        x = func.dropout(x, p=self.p, training=self.training)\n",
    "        x = func.relu(self.conv3(x, edge_index, edge_attr))\n",
    "\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.lin1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "b5a0663c-15ee-45f9-970a-3a4e53041501",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (1) to match target batch_size (0).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[282], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m min_v_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39minf    \n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m21\u001b[39m):\n\u001b[0;32m---> 45\u001b[0m     t_loss \u001b[38;5;241m=\u001b[39m \u001b[43mGCN_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     test_sen, test_spe, test_acc, _ \u001b[38;5;241m=\u001b[39m GCN_test(test_loader)\n",
      "Cell \u001b[0;32mIn[282], line 9\u001b[0m, in \u001b[0;36mGCN_train\u001b[0;34m(loader)\u001b[0m\n\u001b[1;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      8\u001b[0m output \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[0;32m----> 9\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/GCN_study/lib/python3.9/site-packages/torch/nn/functional.py:3026\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3024\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3025\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (1) to match target batch_size (0)."
     ]
    }
   ],
   "source": [
    "\n",
    "def GCN_train(loader):\n",
    "    model.train()\n",
    "\n",
    "    loss_all = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = func.cross_entropy(output, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_all += data.num_graphs * loss.item()\n",
    "        #optimizer.step()\n",
    "    return loss_all / len(train_dataset)\n",
    "\n",
    "\n",
    "def GCN_test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    pred = []\n",
    "    label = []\n",
    "    loss_all = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        output = model(data)\n",
    "        loss = func.cross_entropy(output, data.y)\n",
    "        loss_all += data.num_graphs * loss.item()\n",
    "        pred.append(func.softmax(output, dim=1).max(dim=1)[1])\n",
    "        label.append(data.y)\n",
    "\n",
    "    y_pred = torch.cat(pred, dim=0).cpu().detach().numpy()\n",
    "    y_true = torch.cat(label, dim=0).cpu().detach().numpy()\n",
    "    tn, fp, fn, tp = confusion_matrix(y_pred, y_true).ravel()\n",
    "    epoch_sen = tp / (tp + fn)\n",
    "    epoch_spe = tn / (tn + fp)\n",
    "    epoch_acc = (tn + tp) / (tn + tp + fn + fp)\n",
    "    return epoch_sen, epoch_spe, epoch_acc, loss_all / len(val_dataset)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN(190, 2, 3).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "   \n",
    "min_v_loss = np.inf    \n",
    "for epoch in range(1,21):\n",
    "    t_loss = GCN_train(train_data)\n",
    "    test_sen, test_spe, test_acc, _ = GCN_test(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce366581-87c5-440a-9132-4bd0fb7ab259",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "3ddc34e8-432a-4378-b97b-7a6cbe23ced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_adjacency_matrices = [correlation_matrix(190,X_train.iloc[i,:].values) for i in range((X_test.shape[0]))]  # Your list of training adjacency matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "04e26aed-9026-4f31-bcd8-8041e5254114",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'edge_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[350], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m200\u001b[39m):\n\u001b[1;32m     35\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 36\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_adjacency_matrices\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(out, torch\u001b[38;5;241m.\u001b[39mtensor(train_labels)\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     38\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/GCN_study/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[350], line 13\u001b[0m, in \u001b[0;36mGCN.forward\u001b[0;34m(self, adjacency_matrix)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, adjacency_matrix):\n\u001b[1;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(adjacency_matrix\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mto(adjacency_matrix\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m#x = self.conv1(x, adjacency_matrix)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/GCN_study/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'edge_index'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, adjacency_matrix):\n",
    "        x = torch.eye(adjacency_matrix.shape[0]).to(adjacency_matrix.device)\n",
    "        x = self.conv1(x)\n",
    "        #x = self.conv1(x, adjacency_matrix)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        #x = self.conv2(x, adjacency_matrix)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "train_adjacency_matrices = [torch.tensor(correlation_matrix(190,X_train.iloc[i,:].values)) for i in range((X_train.shape[0]))]  # Your list of training adjacency matrices\n",
    "test_adjacency_matrices = [torch.tensor(correlation_matrix(190,X_test.iloc[i,:].values)) for i in range((X_test.shape[0]))]  # Your list of training adjacency matrices\n",
    "\n",
    "test_labels = y_test.values\n",
    "train_labels = y_train.values\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN(num_features=train_adjacency_matrices[0].shape[0], hidden_dim=64, num_classes=num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(torch.stack(train_adjacency_matrices).to(device))\n",
    "    loss = F.nll_loss(out, torch.tensor(train_labels).to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "out = model(torch.stack(test_adjacency_matrices).to(device))\n",
    "pred = out.argmax(dim=1)\n",
    "acc = int((pred == torch.tensor(test_labels).to(device)).sum()) / len(test_labels)\n",
    "print('Test accuracy: {:.4f}'.format(acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "6793e191-187a-4dbc-bc1f-4fbdcad19328",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (2026768228.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[343], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    for i in DataLoader(train_adjacency_matrices):\u001b[0m\n\u001b[0m                                                  ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "for i in DataLoader(train_adjacency_matrices):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fae5c3-5ef1-4663-8acc-7df17deed7c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
