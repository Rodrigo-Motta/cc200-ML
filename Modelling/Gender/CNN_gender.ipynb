{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd30d74-cc3c-4071-b642-85ea8036965e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /Users/rodrigo/Post-Grad/CC400/Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4725120c-06d7-422b-8bb3-b0349323dbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdd7961-edd8-40bf-8cbf-85b21ca7e1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from neurocombat_sklearn import CombatModel\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "np.random.seed(42)\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(rc={'image.cmap': 'coolwarm'})\n",
    "\n",
    "#from numba import jit,prange\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "SMALL_SIZE = 8\n",
    "MEDIUM_SIZE = 14\n",
    "BIGGER_SIZE = 20\n",
    "\n",
    "plt.rc('font', size=MEDIUM_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=BIGGER_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "scoring = {'acc': 'accuracy',\n",
    "           'prec_macro': 'precision_macro',\n",
    "           'rec_macro': 'recall_macro',\n",
    "          'f1' : 'f1_macro',\n",
    "          'roc_auc' : 'roc_auc'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dee427-324a-4098-bece-206788318379",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, phenotypic = ut.import_data(fisher=False)\n",
    "\n",
    "#df = df.join(pd.DataFrame(phenotypic.Age), how='left')\n",
    "#df = df.join(pd.DataFrame(phenotypic['ADHD Measure']), how='left')\n",
    "df = df.join(pd.DataFrame(phenotypic['Gender']), how='left')\n",
    "\n",
    "df['Age'] = phenotypic['Age']\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "df = shuffle(df, random_state=42)\n",
    "\n",
    "df = df.dropna(axis=0)\n",
    "df = df.reset_index()\n",
    "df['Site'] = df['Institution'].astype('category')\n",
    "df['Site'] = df['Site'].cat.codes\n",
    "\n",
    "# TEST = df[df.Subject.isin(df['Subject'].unique()[-20:])].reset_index()\n",
    "# X_TEST = TEST.drop(columns=['Institution', 'Subject', 'Run','Gender', 'Age', 'Site','index'])\n",
    "# y_TEST = TEST.Gender\n",
    "\n",
    "# df = df[~df.Subject.isin(df['Subject'].unique()[-20:])].reset_index().drop(columns='index')\n",
    "\n",
    "#X = df.drop(columns=['Institution', 'Run', 'Age','ADHD Measure', 'Gender', 'Subject'])\n",
    "y = df.Gender.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b177f58-5935-4c63-8aac-73aa246666e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[~df.Subject.isin(np.random.choice(df['Subject'].unique(),100))]\n",
    "df_test = df[df.Subject.isin(np.random.choice(df['Subject'].unique(),100))]\n",
    "\n",
    "Age_train = df_train[['Age']]\n",
    "Site_train = df_train[['Site']]\n",
    "X_train = df_train.drop(columns=['Institution', 'Subject', 'Run','Gender', 'Age', 'Site', 'Half'])\n",
    "y_train = df_train.Gender\n",
    "\n",
    "Age_test = df_test[['Age']]\n",
    "Site_test = df_test[['Site']]\n",
    "X_test = df_test.drop(columns=['Institution', 'Subject', 'Run', 'Gender', 'Age', 'Site', 'Half'])\n",
    "y_test = df_test.Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f34a173-c505-4bd6-993a-dae1618bad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating model\n",
    "model = CombatModel()\n",
    "\n",
    "# Fitting the model and transforming the training set\n",
    "X_train = model.fit_transform(X_train.values,\n",
    "                                         Site_train) #X_train_har\n",
    "\n",
    "# Harmonize test set using training set fitted parameters\n",
    "X_test = model.transform(X_test.values,\n",
    "                                    Site_test) #X_test_har"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204e4352-f8db-40f8-87a8-015e0b08adeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_train = np.zeros((X_train.shape[0],1, 190, 190))\n",
    "for i in range(X_train.shape[0]):\n",
    "    img_train[i, :, :] = ut.reconstruct_symmetric_matrix(190,X_train[i,:])\n",
    "    \n",
    "img_test = np.zeros((X_test.shape[0], 1, 190, 190))\n",
    "for i in range(X_test.shape[0]):\n",
    "    img_test[i, :, :] = ut.reconstruct_symmetric_matrix(190,X_test[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24692c25-7320-474b-800e-7d6980159f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as func\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None, target_transform=None):\n",
    "        self.images = torch.from_numpy(images)\n",
    "        self.img_labels = torch.from_numpy(labels.values.astype(int))\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        # self.transform = transforms.Compose([\n",
    "        #     transforms.ToTensor(),\n",
    "        #     transforms.Normalize((0.5,), (0.5,))\n",
    "        # ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        #image = self.transform(image)\n",
    "        label = self.img_labels[idx]\n",
    "        return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b0b1c9-3288-49f7-8a4e-0fa1144d4e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_train = ImageDataset(img_train, y_train)\n",
    "train_loader = DataLoader(img_train, batch_size=32)\n",
    "\n",
    "img_test = ImageDataset(img_test, y_test)\n",
    "test_loader = DataLoader(img_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8bf24f-9bd2-4110-90af-3d48e3608caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(img_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7348ad2d-9469-4620-a286-bdd6abecc186",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = img_train[0:4]\n",
    "print(xs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddedd5e4-dc57-446b-8fb9-e7f5f482e641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With square kernels and equal stride\n",
    "m = nn.Conv2d(1, 33, 3, stride=2)\n",
    "n = nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606a1bbb-3c27-4c2b-8e7c-98034b9f4437",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Images .shape \\n', xs.shape)\n",
    "print('Images after CNN .shape \\n',m(xs).shape)\n",
    "print('Images after CNN -> ReLU .shape \\n', n(m(xs)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de126f64-d062-4dcc-86a5-aad7404f83d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self,dropout=.5):\n",
    "        super(NN,self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=(2,2), stride=1)\n",
    "        self.pool1 = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(32, 2, kernel_size=(2,2), stride=1)\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "        self.conv3 = nn.Conv2d(16, 8, kernel_size=(4,4), stride=1)\n",
    "        self.pool3 = nn.MaxPool2d(2,2)\n",
    "        \n",
    "        self.lin1 = nn.Linear(4232, 2)\n",
    "        #self.R = nn.LeakyReLU()\n",
    "        self.R = nn.Tanh()\n",
    "        self.p = dropout\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.R(self.conv1(x))\n",
    "        x = self.R(self.pool1(x))\n",
    "        x = func.dropout(x, p=self.p, training=self.training)\n",
    "        x = self.R(self.conv2(x))\n",
    "        x = self.R(self.pool2(x))\n",
    "        #x = func.dropout(x, p=self.p, training=self.training)\n",
    "        #x = self.R(self.conv3(x))\n",
    "        #x = self.R(self.pool3(x))\n",
    "        #x = x.view(-1,4*(46**2))\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.lin1(x)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b473422-8872-40db-9442-36ccbdeb1091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2bfcbb-31aa-42bf-a3c9-bafda180c88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = NN()\n",
    "count_parameters(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eec289c-e5bb-4de5-8a07-3012eac922a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26655b0-7511-40d6-a1ef-ad8a781eeb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "f(xs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b838d0-e46a-4e43-b287-89c9d9cbd847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch, (X, y) in enumerate(train_loader):\n",
    "#     print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93549731-7fb1-4396-9452-0fa8a0942097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_loader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49080cd-8e9d-4ccd-acd7-264a959ef1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "def train_loop(dataloader, model, optimizer, loop):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "    pred = []\n",
    "    label = []\n",
    "    \n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    for batch, (X, y) in enumerate(loop):\n",
    "        \n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y = y.to(device)\n",
    "        # Compute prediction and loss\n",
    "        output = model(X)\n",
    "        #print(output.shape)\n",
    "        loss = func.cross_entropy(output, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss_all += loss.item()*len(y)\n",
    "        \n",
    "        loop.set_description(f\"Epoch [{epoch}/{NUM_EPOCHS}]\")\n",
    "        loop.set_postfix(loss=loss_all/len(img_train))\n",
    "        \n",
    "        pred.append(func.softmax(output, dim=1).max(dim=1)[1])\n",
    "        label.append(y)\n",
    "\n",
    "        # if batch % 100 == 0:\n",
    "        #     loss, current = loss.item(), (batch + 1) * len(X)\n",
    "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            #optimizer.step()\n",
    "    y_pred = torch.cat(pred, dim=0).cpu().detach().numpy()\n",
    "    y_true = torch.cat(label, dim=0).cpu().detach().numpy()\n",
    "    tn, fp, fn, tp = confusion_matrix(y_pred, y_true).ravel()\n",
    "    epoch_acc = (tn + tp) / (tn + tp + fn + fp)\n",
    "        \n",
    "    return epoch_acc, loss_all / size\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    loss_all = 0\n",
    "    pred = []\n",
    "    label = []\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            output = model(X)\n",
    "            loss = func.cross_entropy(output, y)\n",
    "            correct += (output.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "            loss_all += loss.item()*len(y)\n",
    "            pred.append(func.softmax(output, dim=1).max(dim=1)[1])\n",
    "            label.append(y)\n",
    "\n",
    "    y_pred = torch.cat(pred, dim=0).cpu().detach().numpy()\n",
    "    y_true = torch.cat(label, dim=0).cpu().detach().numpy()\n",
    "    tn, fp, fn, tp = confusion_matrix(y_pred, y_true).ravel()\n",
    "    epoch_rec = tp / (tp + fn)\n",
    "    epoch_prec = tp / (tp + fp)\n",
    "    epoch_f1 = 2*(epoch_rec*epoch_prec)/(epoch_rec + epoch_prec)\n",
    "    epoch_acc = (tn + tp) / (tn + tp + fn + fp)\n",
    "    \n",
    "    # AUC & ROC\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {(loss_all / size):>8f} \\n\")\n",
    "    \n",
    "    return epoch_rec, epoch_prec, epoch_acc, loss_all / size, roc_auc,epoch_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090aa2fc-3391-4901-8eaa-9973a91dc40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\"loss_train\" : [], \"loss_test\" : [], \"acc_test\" : [], \"acc_train\" : []}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = NN().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "L = nn.CrossEntropyLoss()\n",
    "\n",
    "NUM_EPOCHS = 25\n",
    "for epoch in range(1,NUM_EPOCHS + 1):\n",
    "    loop = tqdm(train_loader)\n",
    "    train_acc, train_loss = train_loop(train_loader, model, optimizer, loop)\n",
    "    test_rec, test_prec, test_acc, test_loss, roc_auc, test_f1 = test_loop(test_loader, model)\n",
    "    \n",
    "    \n",
    "    \n",
    "    metrics['loss_train'].append(train_loss)\n",
    "    metrics['loss_test'].append(test_loss)\n",
    "    metrics['acc_test'].append(test_acc)\n",
    "    metrics['acc_train'].append(train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae20e3d-6438-4fa2-b732-a428d80c4296",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for y_i in test_loader:\n",
    "\n",
    "    #y_pred.append(model(y).detach().numpy())\n",
    "    y_pred.append(func.softmax(model(y_i[0]), dim=1).detach().numpy())#.max(dim=1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0516bdc6-f465-4792-9809-74bd8de28ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.hist(np.array(y_pred), bins=15)\n",
    "plt.title(\"Prob distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763c5fee-18e2-4cf3-9a5c-de25c5824c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,6))\n",
    "ax.plot(metrics['loss_train'], label='train')\n",
    "ax.plot(metrics['loss_test'], label='Validation')\n",
    "ax.set_ylabel('Cross entropy loss')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.legend()\n",
    "#plt.xlim(0,100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508a7d2f-f921-4de2-b191-e50580b08bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = ut.cross_val_data(df, folds=5, site=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912ebedd-27ca-4142-974f-34e7fc874b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "eval_scores = {\"loss_train\" : [], \"loss_test\" : [], \"acc_test\" : [], \"acc_train\" : []}\n",
    "\n",
    "scores = np.zeros((6,5))\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "for k in range(5):\n",
    "    \n",
    "#     scaler = StandardScaler()\n",
    "#     X_train[k] = scaler.fit_transform(X_train[k])\n",
    "\n",
    "#     X_test[k] = scaler.transform(X_test[k])\n",
    "\n",
    "    img_train = np.zeros((X_train[k].shape[0],1, 190, 190))\n",
    "    for i in range(X_train[k].shape[0]):\n",
    "        img_train[i, :, :] = ut.reconstruct_symmetric_matrix(190,X_train[k][i,:])\n",
    "\n",
    "    img_test = np.zeros((X_test[k].shape[0], 1, 190, 190))\n",
    "    for i in range(X_test[k].shape[0]):\n",
    "        img_test[i, :, :] = ut.reconstruct_symmetric_matrix(190,X_test[k][i,:])\n",
    "        \n",
    "    img_train = ImageDataset(img_train, y_train[k])\n",
    "    train_loader = DataLoader(img_train, batch_size=64)\n",
    "\n",
    "    img_test = ImageDataset(img_test, y_test[k])\n",
    "    test_loader = DataLoader(img_test, batch_size=64)\n",
    "    \n",
    "    \n",
    "    \n",
    "    metrics = {\"loss_train\" : [], \"loss_test\" : [], \"acc_test\" : [], \"acc_train\" : []}\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = NN().to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "\n",
    "\n",
    "    NUM_EPOCHS = 10\n",
    "    for epoch in range(1,NUM_EPOCHS + 1):\n",
    "        loop = tqdm(train_loader)\n",
    "        train_acc, train_loss = train_loop(train_loader, model, optimizer, loop)\n",
    "        test_rec, test_prec, test_acc, test_loss, roc_auc, test_f1 = test_loop(test_loader, model)\n",
    "\n",
    "\n",
    "\n",
    "        eval_scores['loss_train'].append(train_loss)\n",
    "        eval_scores['loss_test'].append(test_loss)\n",
    "        eval_scores['acc_test'].append(test_acc)\n",
    "        eval_scores['acc_train'].append(train_acc)\n",
    "        \n",
    "        print('Val Accuracy {} , Val Loss {}'.format(test_acc, test_loss))\n",
    "        print('Train Accuracy {} , Train Loss {}'.format(train_acc, train_loss))\n",
    "\n",
    "    \n",
    "    scores[0][fold_no - 1] = test_acc\n",
    "    scores[1][fold_no - 1] = test_rec\n",
    "    scores[2][fold_no - 1] = test_prec\n",
    "    scores[3][fold_no - 1] = test_loss\n",
    "    scores[4][fold_no - 1] = roc_auc\n",
    "    scores[5][fold_no - 1] = test_f1\n",
    "\n",
    "    print(f'Score for fold {fold_no}: loss of {test_loss}; acc of {test_acc}%; AUC of {roc_auc}%')\n",
    "\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccec57a-cf3f-43cc-a86f-91a263857659",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
